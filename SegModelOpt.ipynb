{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing a U-Net for Segmentation\n",
    "\n",
    "This notebook explores optimization techniques for a U-Net model designed for image segmentation. We'll implement:\n",
    "- **Quantization**: Reducing model precision for efficiency.\n",
    "- **Pruning**: Removing unnecessary weights to slim down the model.\n",
    "- **Mixed Precision**: Using lower-precision types to speed up training and inference.\n",
    "\n",
    "The dataset used is the Oxford-IIIT Pet Dataset, and all experiments run in a Conda environment with Python 3.10.16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Preparation\n",
    "\n",
    "### Dataset\n",
    "- **Source**: [Oxford-IIIT Pet Dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/)\n",
    "- **Path**: Set `DATASET_PATH` to your local directory containing `images` and `annotations/trimaps`.\n",
    "\n",
    "Let’s load the libraries and prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, Model\n",
    "import psutil\n",
    "from tensorflow.keras import backend as K\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import tensorflow_model_optimization as tfmot\n",
    "\n",
    "# Constants\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 5\n",
    "DATASET_PATH = \"oxford-iiit-pet\"  # Update this to your dataset directory\n",
    "IMAGES_PATH = os.path.join(DATASET_PATH, \"images\")\n",
    "MASKS_PATH = os.path.join(DATASET_PATH, \"annotations/trimaps\")\n",
    "\n",
    "# Load image and mask file lists\n",
    "image_files = sorted([os.path.join(IMAGES_PATH, f) for f in os.listdir(IMAGES_PATH) if f.endswith(\".jpg\")])\n",
    "mask_files = sorted([os.path.join(MASKS_PATH, f) for f in os.listdir(MASKS_PATH) if f.endswith(\".png\")])\n",
    "\n",
    "# Verify matching pairs\n",
    "assert len(image_files) == len(mask_files), \"Mismatch between images and masks!\"\n",
    "print(f\"Total images: {len(image_files)}, Total masks: {len(mask_files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Visualization\n",
    "\n",
    "Here, we define functions to load and preprocess images and masks, plus a quick way to visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image_and_mask(image_path, mask_path):\n",
    "    \"\"\"Load and preprocess an image-mask pair.\"\"\"\n",
    "    # Image\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, IMG_SIZE) / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "    # Mask\n",
    "    mask = tf.io.read_file(mask_path)\n",
    "    mask = tf.io.decode_png(mask, channels=1)\n",
    "    mask = tf.image.resize(mask, IMG_SIZE, method=\"nearest\")\n",
    "    mask = tf.where(mask > 1, 1, 0)  # Binary: 0 = background, 1 = pet\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "def display_sample(dataset):\n",
    "    \"\"\"Show a sample of 5 image-mask pairs from the dataset.\"\"\"\n",
    "    for images, masks in dataset.take(1):\n",
    "        fig, ax = plt.subplots(2, 5, figsize=(15, 6))\n",
    "        for i in range(5):\n",
    "            ax[0, i].imshow(images[i])\n",
    "            ax[0, i].set_title(\"Image\")\n",
    "            ax[0, i].axis(\"off\")\n",
    "            ax[1, i].imshow(masks[i].numpy().squeeze(), cmap=\"gray\")\n",
    "            ax[1, i].set_title(\"Mask\")\n",
    "            ax[1, i].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Split into train and validation sets\n",
    "def train_val_split(image_files, mask_files, val_ratio=0.2):\n",
    "    \"\"\"Create train and validation datasets.\"\"\"\n",
    "    image_train, image_val, mask_train, mask_val = train_test_split(\n",
    "        image_files, mask_files, test_size=val_ratio, random_state=42, shuffle=True\n",
    "    )\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((image_train, mask_train))\n",
    "    train_dataset = train_dataset.map(load_image_and_mask, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    train_dataset = train_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((image_val, mask_val))\n",
    "    val_dataset = val_dataset.map(load_image_and_mask, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    val_dataset = val_dataset.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "# Create datasets\n",
    "train_dataset, val_dataset = train_val_split(image_files, mask_files)\n",
    "display_sample(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net Model Definition\n",
    "\n",
    "This is our base U-Net architecture for binary segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet(input_shape=(128, 128, 3)):\n",
    "    \"\"\"Build a U-Net model with skip connections.\"\"\"\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    # Encoder\n",
    "    c1 = layers.Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\")(inputs)\n",
    "    c1 = layers.Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\")(c1)\n",
    "    p1 = layers.MaxPooling2D((2, 2))(c1)\n",
    "\n",
    "    c2 = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(p1)\n",
    "    c2 = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(c2)\n",
    "    p2 = layers.MaxPooling2D((2, 2))(c2)\n",
    "\n",
    "    # Bottleneck\n",
    "    c3 = layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(p2)\n",
    "    c3 = layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(c3)\n",
    "\n",
    "    # Decoder\n",
    "    u4 = layers.UpSampling2D((2, 2))(c3)\n",
    "    u4 = layers.Concatenate()([u4, c2])\n",
    "    c4 = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(u4)\n",
    "    c4 = layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(c4)\n",
    "\n",
    "    u5 = layers.UpSampling2D((2, 2))(c4)\n",
    "    u5 = layers.Concatenate()([u5, c1])\n",
    "    c5 = layers.Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\")(u5)\n",
    "    c5 = layers.Conv2D(16, (3, 3), activation=\"relu\", padding=\"same\")(c5)\n",
    "\n",
    "    outputs = layers.Conv2D(1, (1, 1), activation=\"sigmoid\")(c5)\n",
    "\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "# Build and summarize the base model\n",
    "base_model = build_unet()\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "We’ll use these metrics to evaluate model quality and performance:\n",
    "- **Accuracy**: Pixel-wise correctness.\n",
    "- **Dice Coefficient**: Overlap between prediction and ground truth.\n",
    "- **IoU**: Intersection over Union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_true, y_pred, threshold=0.5):\n",
    "    \"\"\"Calculate pixel-wise accuracy.\"\"\"\n",
    "    y_pred = tf.cast(y_pred > threshold, tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    correct = tf.reduce_sum(tf.cast(tf.equal(y_true, y_pred), tf.float32))\n",
    "    total = tf.reduce_prod(tf.cast(tf.shape(y_true), tf.float32))\n",
    "    return correct / total\n",
    "\n",
    "def compute_dice(y_true, y_pred, threshold=0.5):\n",
    "    \"\"\"Calculate Dice coefficient.\"\"\"\n",
    "    y_pred = tf.cast(y_pred > threshold, tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    return (2. * intersection + K.epsilon()) / (tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) + K.epsilon())\n",
    "\n",
    "def compute_iou(y_true, y_pred, threshold=0.5):\n",
    "    \"\"\"Calculate Intersection over Union (IoU).\"\"\"\n",
    "    y_pred = tf.cast(y_pred > threshold, tf.float32)\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred) - intersection\n",
    "    return intersection / (union + K.epsilon())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking Functions\n",
    "\n",
    "These functions measure model size, inference time, memory usage, and quality metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_keras_model(model, val_dataset, num_samples=100):\n",
    "    \"\"\"Benchmark a Keras model on performance and quality.\"\"\"\n",
    "    # Model size\n",
    "    model_path = \"unet_model.keras\"\n",
    "    model.save(model_path)\n",
    "    model_size = os.path.getsize(model_path) / (1024 * 1024)  # MB\n",
    "\n",
    "    # Prepare test data\n",
    "    test_images, test_masks = [], []\n",
    "    for img, mask in val_dataset.take(num_samples // BATCH_SIZE):\n",
    "        test_images.append(img.numpy())\n",
    "        test_masks.append(mask.numpy())\n",
    "    test_images = np.vstack(test_images)\n",
    "    test_masks = np.vstack(test_masks)\n",
    "\n",
    "    # Benchmark\n",
    "    inference_times, accuracies, dices, ious = [], [], [], []\n",
    "    process = psutil.Process()\n",
    "    mem_before = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        input_data = test_images[i:i+1]\n",
    "        start_time = time.time()\n",
    "        output = model.predict(input_data, verbose=0)\n",
    "        inference_times.append(time.time() - start_time)\n",
    "\n",
    "        accuracies.append(compute_accuracy(test_masks[i], output))\n",
    "        dices.append(compute_dice(test_masks[i], output))\n",
    "        ious.append(compute_iou(test_masks[i], output))\n",
    "\n",
    "    mem_after = process.memory_info().rss / (1024 * 1024)\n",
    "    return {\n",
    "        \"model_size_mb\": model_size,\n",
    "        \"avg_inference_time_ms\": np.mean(inference_times) * 1000,\n",
    "        \"memory_usage_mb\": mem_after - mem_before,\n",
    "        \"avg_accuracy\": np.mean(accuracies),\n",
    "        \"avg_dice\": np.mean(dices),\n",
    "        \"avg_iou\": np.mean(ious)\n",
    "    }\n",
    "\n",
    "def benchmark_tflite_model(tflite_path, val_dataset, num_samples=100):\n",
    "    \"\"\"Benchmark a TFLite model on performance and quality.\"\"\"\n",
    "    # Model size\n",
    "    model_size = os.path.getsize(tflite_path) / (1024 * 1024)  # MB\n",
    "\n",
    "    # Load TFLite model\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Prepare test data\n",
    "    test_images, test_masks = [], []\n",
    "    for img, mask in val_dataset.take(num_samples // BATCH_SIZE):\n",
    "        test_images.append(img.numpy())\n",
    "        test_masks.append(mask.numpy())\n",
    "    test_images = np.vstack(test_images)\n",
    "    test_masks = np.vstack(test_masks)\n",
    "\n",
    "    # Benchmark\n",
    "    inference_times, accuracies, dices, ious = [], [], [], []\n",
    "    process = psutil.Process()\n",
    "    mem_before = process.memory_info().rss / (1024 * 1024)  # MB\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        input_data = test_images[i:i+1].astype(np.float32)\n",
    "        interpreter.set_tensor(input_details[0][\"index\"], input_data)\n",
    "        start_time = time.time()\n",
    "        interpreter.invoke()\n",
    "        output = interpreter.get_tensor(output_details[0][\"index\"])\n",
    "        inference_times.append(time.time() - start_time)\n",
    "\n",
    "        accuracies.append(compute_accuracy(test_masks[i], output))\n",
    "        dices.append(compute_dice(test_masks[i], output))\n",
    "        ious.append(compute_iou(test_masks[i], output))\n",
    "\n",
    "    mem_after = process.memory_info().rss / (1024 * 1024)\n",
    "    return {\n",
    "        \"model_size_mb\": model_size,\n",
    "        \"avg_inference_time_ms\": np.mean(inference_times) * 1000,\n",
    "        \"memory_usage_mb\": mem_after - mem_before,\n",
    "        \"avg_accuracy\": np.mean(accuracies),\n",
    "        \"avg_dice\": np.mean(dices),\n",
    "        \"avg_iou\": np.mean(ious)\n",
    "    }\n",
    "\n",
    "def print_benchmark_results(model_name, results):\n",
    "    \"\"\"Display benchmark results in a formatted way.\"\"\"\n",
    "    print(f\"\\n=== {model_name} Benchmark Results ===\")\n",
    "    print(f\"Model Size: {results['model_size_mb']:.2f} MB\")\n",
    "    print(f\"Avg Inference Time: {results['avg_inference_time_ms']:.2f} ms\")\n",
    "    print(f\"Memory Usage: {results['memory_usage_mb']:.2f} MB\")\n",
    "    print(f\"Accuracy: {results['avg_accuracy']:.4f}\")\n",
    "    print(f\"Dice Coefficient: {results['avg_dice']:.4f}\")\n",
    "    print(f\"IoU: {results['avg_iou']:.4f}\")\n",
    "    print(\"=====================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Base Model\n",
    "\n",
    "Let’s train the original U-Net to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train base model\n",
    "base_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "base_model.fit(train_dataset, epochs=5, validation_data=val_dataset)\n",
    "\n",
    "# Benchmark\n",
    "results_original = benchmark_keras_model(base_model, val_dataset)\n",
    "print_benchmark_results(\"Original U-Net\", results_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization Techniques\n",
    "\n",
    "### 1. Post-Training Quantization (INT8)\n",
    "\n",
    "Convert the model to INT8 for smaller size and faster inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TFLite with INT спря8 quantization\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(base_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# Save\n",
    "with open(\"unet_quant_int8.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_quant_model)\n",
    "\n",
    "# Benchmark\n",
    "results_quantized_int8 = benchmark_tflite_model(\"unet_quant_int8.tflite\", val_dataset)\n",
    "print_benchmark_results(\"Quantized (INT8)\", results_quantized_int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Quantization-Aware Training (QAT)\n",
    "\n",
    "Train the model with quantization in mind for better accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply QAT\n",
    "quantize_model = tfmot.quantization.keras.quantize_model\n",
    "qat_model = quantize_model(base_model)\n",
    "qat_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "qat_model.fit(train_dataset, epochs=5, validation_data=val_dataset)\n",
    "\n",
    "# Convert to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(qat_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_qat_model = converter.convert()\n",
    "\n",
    "# Save\n",
    "with open(\"unet_qat.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_qat_model)\n",
    "\n",
    "# Benchmark\n",
    "results_qat = benchmark_tflite_model(\"unet_qat.tflite\", val_dataset)\n",
    "print_benchmark_results(\"Quantization-Aware Training\", results_qat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Pruning\n",
    "\n",
    "Prune the model to reduce its complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pruning parameters\n",
    "pruning_params = {\n",
    "    \"pruning_schedule\": tfmot.sparsity.keras.PolynomialDecay(\n",
    "        initial_sparsity=0.0, final_sparsity=0.5, begin_step=0, end_step=1000\n",
    "    )\n",
    "}\n",
    "\n",
    "# Apply pruning\n",
    "pruned_model = tfmot.sparsity.keras.prune_low_magnitude(base_model, **pruning_params)\n",
    "pruned_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "pruned_model.fit(train_dataset, epochs=1, callbacks=[tfmot.sparsity.keras.UpdatePruningStep()])\n",
    "\n",
    "# Strip pruning wrappers\n",
    "pruned_model = tfmot.sparsity.keras.strip_pruning(pruned_model)\n",
    "\n",
    "# Convert to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(pruned_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_pruned_model = converter.convert()\n",
    "\n",
    "# Save\n",
    "with open(\"unet_pruned.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_pruned_model)\n",
    "\n",
    "# Benchmark\n",
    "results_pruned = benchmark_tflite_model(\"unet_pruned.tflite\", val_dataset)\n",
    "print_benchmark_results(\"Pruned\", results_pruned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Mixed Precision\n",
    "\n",
    "Use mixed precision to speed up computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable mixed precision\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# Build and train\n",
    "mixed_model = build_unet()\n",
    "mixed_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "mixed_model.fit(train_dataset, epochs=1, validation_data=val_dataset)\n",
    "\n",
    "# Benchmark\n",
    "results_mixed = benchmark_keras_model(mixed_model, val_dataset)\n",
    "print_benchmark_results(\"Mixed Precision\", results_mixed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary\n",
    "\n",
    "Let’s compare all models in a nice table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile results\n",
    "results_dict = {\n",
    "    \"Original\": results_original,\n",
    "    \"Quantized (INT8)\": results_quantized_int8,\n",
    "    \"QAT\": results_qat,\n",
    "    \"Pruned\": results_pruned,\n",
    "    \"Mixed Precision\": results_mixed\n",
    "}\n",
    "\n",
    "# Create DataFrame\n",
    "df_results = pd.DataFrame.from_dict(results_dict, orient=\"index\").reset_index()\n",
    "df_results.rename(columns={\"index\": \"Model\"}, inplace=True)\n",
    "\n",
    "# Style the table\n",
    "styled_df = df_results.style.set_table_styles([\n",
    "    {\"selector\": \"th\", \"props\": [(\"text-align\", \"center\"), (\"font-weight\", \"bold\")]},\n",
    "    {\"selector\": \"td\", \"props\": [(\"text-align\", \"center\")]}\n",
    "]).format({\n",
    "    \"model_size_mb\": \"{:.2f}\",\n",
    "    \"avg_inference_time_ms\": \"{:.2f}\",\n",
    "    \"memory_usage_mb\": \"{:.2f}\",\n",
    "    \"avg_accuracy\": \"{:.4f}\",\n",
    "    \"avg_dice\": \"{:.4f}\",\n",
    "    \"avg_iou\": \"{:.4f}\"\n",
    "}).hide(axis=\"index\")\n",
    "\n",
    "# Display\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook tested various optimization strategies on a U-Net for pet segmentation. Quantization and pruning reduced model size and inference time, while mixed precision offered a balance of speed and resource use. Adjust epochs and parameters as needed for your specific use case!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
